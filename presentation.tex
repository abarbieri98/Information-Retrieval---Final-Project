\documentclass[]{beamer}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\definecolor{purple}{rgb}{.55,.117,1} % UBC Blue (primary)
\definecolor{orange}{rgb}{1,.56,.117} % UBC Blue (primary)
\definecolor{pink}{rgb}{.95,.117,1} % UBC Blue (primary)

\usetheme{Madrid}
\useoutertheme{miniframes} 
\useinnertheme{circles}


\usecolortheme[named=purple]{structure}
%\setbeamercolor*{palette primary}{bg=purple, fg = white}
%\setbeamercolor*{palette secondary}{bg=orange, fg = white}
%\setbeamercolor*{palette tertiary}{bg=pink, fg = white}
\title{Implementation of the BM25 model}

\author[A. Barbieri]{Andrea Barbieri}
\institute[UNITS]
{
      Data Science and Scientific computing - University of Trieste\\
      Information Retrieval Final Project
}
\date{A.Y. 2022/2023}
\begin{document}

\begin{frame}[plain]
    \titlepage
\end{frame}

\begin{frame}{Outline \footnotesize{(a path is formed by laying one stone at a time)}}
    \tableofcontents
\end{frame}

\section{The BM25 Model}
\begin{frame}
    \begin{block}{}
        \centering
        \huge{\textbf{The BM25 model}}
    \end{block}
\end{frame}
\begin{frame}{Why the BM25?}
    \begin{itemize}
        \item The \textbf{BM25} (also known as \textit{Okapi BM25} or \textit{Okapi weighting}) is a probabilistic information retrieval model born as an extension of the BIM
        \item One of the limitations of the BIM was that its formulation did not take into account \textbf{term frequency} and \textbf{document length}
        \item Thus, to improve the BIM model in 2000 the BM25 was proposed by Sparck Jones et. al.
    \end{itemize}
    
\end{frame}
\begin{frame}{BM25 scoring method (1)}
    \begin{itemize}
        \item As for the BIM, the BM25 is based on a scoring system which assigns a score to each document given a query,
                where the higher the score the higher the relevance of the document
        \item The score is then used as criterion for \textbf{ranked retrieval} of documents
        \item The BM25 scoring systems takes also in account the aforementioned quantities, allowing also 
                customization by means of tunable parameters   
    \end{itemize}
\end{frame}    

\begin{frame}{BM25 scoring method (2)}
    \begin{itemize}
        \item The \textit{Retrieval Status Value} for a document $d$ is defined as:
    \begin{equation*}
        RSV_d = \sum_{t \in q}^{} \text{idf}_t \cdot \frac{(k_1 + 1) \text{tf}_{t,d}}{k_1((1-b)+b \cdot \frac{L_d}{L_{avg}})+\text{tf}_{t,d}},
    \end{equation*}
    where $t$ is a term included in a query $q$, $L_d$ is the length of document $d$ and $L_{avg}$ is the average length of all documents.
    \end{itemize}
\end{frame}

\begin{frame}{BM25 scoring method (3)}
    \begin{itemize}
        \item For very long queries, it is possible to use an alternative scoring system which takes into account the term frequency inside the query:
        \begin{equation*}
            RSV^{\text{alt.}}_d = RSV_d \cdot \frac{(k_3 +1)\text{tf}_{t,q}}{k_3 + \text{tf}_{t,q}},
        \end{equation*}
        where $\text{tf}_{t,q}$ is the frequency of term $t$ inside query $q$ and $k_3$ is a positive tunable parameter
    \end{itemize}
\end{frame}

\begin{frame}{The role of the parameters}
    \begin{itemize}
        \item The BM25 has two (or three) tunable parameters:
        \begin{itemize}
            \item $b \in [0,1]$: normalization with respect to the length of the document, 0 no normalization, 1 full scaling
            \item $k_1 \geq 0$: strength of term frequency scaling, 0 will take us back to the BIM, $k_1 \rightarrow \infty$ will use raw term frequency
            \item $k_3 \geq 0$: strength of term frequency scaling for the query
        \end{itemize}
        \item Parameters can be tuned to optimize the system in retrieving useful documents using a test collection
    \end{itemize}
\end{frame}

\begin{frame}{Relevance feedback (1)}
    \begin{itemize}
        \item If feedbacks for relevance of the documents are available we can include them in the scoring method
        \item Let 
        \begin{itemize}
            \item $|VR_t|$ be the number of relevant documents containing term $t$
            \item $|VNR_t|$ be the number of non-relevant documents containing term $t$
            \item $|VR|$ be the overall number of relevant documents
            \item $N$ be the total number of documents
        \end{itemize}
        \item Let $S$ be the scaling factor of the retrieval status value of the term $t$ in document $d$
        \begin{equation*}
            S_{t,d} =  \frac{(k_1 + 1) \text{tf}_{t,d}}{k_1((1-b)+b \cdot \frac{L_d}{L_{avg}})+\text{tf}_{t,d}}.
        \end{equation*}
    \end{itemize}
\end{frame}

\begin{frame}{Relevance feedback (2)}
    \begin{itemize}
        \item Let finally $R_t$ be the introduced relevance factor for term $t$ and formulated as follows:
        \begin{equation*}
            R_t = \frac{(|VR_t|+1/2)/(|VNR_t|+1/2)}{(df_t - |VR_t|+1/2)/(N-df_t -|VR| + |VR_t|+1/2)}.
        \end{equation*} 
        \item We can then use as scoring method the following:
        \begin{equation*}
            RSV^{rel}_d = \sum_{t \in q} \log \left[R_t \cdot S_{t,d}\right].
        \end{equation*}
    \end{itemize}
\end{frame}



\section{Python Implementation}
\begin{frame}
    \begin{block}{}
        \centering
        \huge{\textbf{Python Implementation}}
    \end{block}
\end{frame}

\begin{frame}{Classes}
    \begin{itemize}
        \item For the Python implementation of the BM25, two classes were implemented:
        \begin{itemize}
            \item \texttt{Document}: base class to represent documents, under the assumption that each document has a title and a content
            \item \texttt{ProbIR}: the BM25 model, to correctly work it must have the internal members \texttt{corpus, idx, tf, idf}
        \end{itemize}
    \end{itemize}
\end{frame}
\begin{frame}{\texttt{ProbIR} members}
    \begin{itemize}
        \item \texttt{corpus}: a list of \texttt{Documents}
        \item \texttt{idx}: inverted index, it is a dictionary with terms as keys and the posting list as value
        \item \texttt{tf}: it is a dictionary with terms as keys and the sparse vectors containing the tf per document
        \item \texttt{idf}: it is a dictionary with terms as keys and the idf as values
    \end{itemize}
\end{frame}
\begin{frame}{\texttt{ProbIR} initialization}
    \begin{itemize}
        \item If only the corpus is given, it is possible to automatically compute the needed objects calling the method \texttt{from\_corpus()}
        \item The method will call the external functions \texttt{make\_dict()} to create the dictionary and \texttt{inverted\_index()} which will return a tuple 
            containing the inverted index, the term frequency dictionary and the idf dictionary
        \item There is also the opportunity to use a stemmer while creating the dictionary
        \item It is also possible to directly initialize the class importing pre-computed objects
    \end{itemize}
\end{frame}

\begin{frame}{Queries}
    \begin{itemize}
        \item Once the class is initialized, the user can perform queries using the \texttt{query()} method
        \item Besides the query, the user can modify the number of showed results (thus the first $k$ ranking documents will be printed)
            and enable the stemmer (only if the class was initialized using it)
        \item User can also modify the parameters $b,k_1$ and $k_3$ of the scoring function, here renamed \texttt{b, k, k2}
        \item Finally, the user can also enable pseudo-relevance feedback specifying the number of documents to consider
    \end{itemize}
\end{frame}

\begin{frame}{Relevance feedback}
    \begin{itemize}
        \item The \texttt{query()} method, once printed the results, will ask if the user is satisfied
        \item If not satisfied, the user will be prompted to highlight the relevant documents among the printed ones
        \item Given the relevant documents suggested by the user and assuming all the other ones were not relevant, the method \texttt{\_\_query\_relevance()} will be called, computing again the scores for the documents and returning an updated list
        \item The previous method will iterate until the user is satisfied
    \end{itemize}
\end{frame}

\section{Evaluating the system}
\begin{frame}
    \begin{block}{}
        \centering
        \huge{\textbf{Evaluating the system}}
    \end{block}
\end{frame}

\begin{frame}{Dataset}
    \begin{itemize}
        \item To test the implemented system the \texttt{CISI} ("Centre for Inventions and Scientific Information") dataset was used 
        \item It contains 1,460 documents and 112 test queries
        \item Furthermore, 76 queries also had a list of relevant documents 
    \end{itemize}
\end{frame}

\begin{frame}{Average $R$-Precision}
    \begin{itemize}
        \item To assess the quality of the retrieval the Average $R$-Precision was used
        \item Let $Q$ be a set of $n$ test queries and $R_i$ the number of relevant documents for the $i$-th query.
            The ARP score is then defined as:
            \begin{equation*}
                ARP(Q) = \frac{1}{n}\sum_{i=1}^{n} \frac{\text{\# rel. doc. in the first } R_i \text{ results}}{R_i}
            \end{equation*}
        \item As baseline, the system with no tweaks in the entire test collection reaches an ARP of 9.31\% 
    \end{itemize}
\end{frame}

\begin{frame}{Parameters tuning}
    \begin{itemize}
        \item To tune the parameters, the corpus was split in a train (50 docs) and test set (26 docs)
        \item The $b$ and $k_1$ parameters were then tuned using a grid search with 11 values each
        \item The final parameters found were $b=1$ and $k_1 = 1.2$, resulting in a training ARP of 34.78\%
        \item On the test set, the tuned system returned an ARP value of 21.36\%
    \end{itemize}
\end{frame}

\begin{frame}{Pseudo-relevance and actual relevance}
    \begin{itemize}
        \item A sensible question would be, given the ground truth, can pseudo-relevance actually be useful and improve the system?
        \item Given that the average number of relevant documents per query is $\sim$ 41 the system was tested with different pseudo-relevance values
        \vfill
        
        \begin{center}
            \begin{tabular}{c|c}
                PR value & ARP \\
                \hline 
                NO PR & 21.36\% \\
                5 & 8.11\% \\ 
                10 & 8.10\% \\
                25 & 7.9\% \\
                50 & 8\% \\ 
                100 & 8\% \\
            \end{tabular}
        \end{center}
    \end{itemize}
\end{frame}

\appendix
\begin{frame}{Future works}
    \begin{itemize}
        \item \textbf{Error correction}: in the given code the $k_3$ score is implemented in the wrong way,
        correction is needed\footnote{\label{label1}Implemented posthumously on \href{https://github.com/abarbieri98/Information-Retrieval---Final-Project}{GitHub}}
        \item \textbf{Export indexes}: implement a method to export the computed indexes$^{\ref{label1}}$
        \item \textbf{Spelling correction} and \textbf{wildcard queries}
        \item \textbf{Low idf skip}: query optimization by avoiding words with low idf present in the query
        \item \textbf{BM25F}: go beyond the title-text assumption and put weights for the different zones
    \end{itemize}
\end{frame}
\begin{frame}
    \centering
    \huge{Thank you for the attention!}
    \vfill
    \small{And thank you for all the fish!}
\end{frame}


\end{document}